**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#5:        |
| ------------------|
| Ethan Rigby       |
| Labib Afsar Ahmed |
| Mohammed Alshoura |
| Danielle Jourdain |

# Introduction

The purpose of this lab was to give us experience assessing the reliability of different systems when given the failure data retrieved from integration testing. This lab aims to further our understanding of reliability growth testing and SUT reliability by allowing us to use tools to analyze this data. The tools we could choose to use in this lab were SRTAT, C-SFRAT, and/or RDC-11. These were used to plot the failure rates and reliability of the SUTs. Then, the results will be compared to deepen our understanding of these tests.

# Assessment Using Reliability Growth Testing

For the Reliability Growth Testing, the Failure Report 10 file was using found inside the Failure Count folder. This data was transfered from the .DAT format into a .csv file in order to allow it to be compatible with the C-SFRAT program.

# Assessment Using Reliability Demonstration Chart

For the Reliability Demonstration Chart, we used Failure Report 10. The data was translated from its tabular form into the format needed for the Excel sheet. Each column was 1 failure and the time was calculated based on a base time. We chose to use RDC-11 to plot the reliability demonstration chart. Due to the limit of 16 data points, we plotted the first 16 failures of the system.

We chose the risk values as follows:

- Discrimination ratio: 2
- Developer's risk: 0.1
- User's risk: 0.1

We found the minimum MTTF by running several different test cases to see what produced an acceptable graph. For this data set, we found that 6 failures per 1,000,000 was the maximum amount of failure that was acceptable. The calculation for MTTF, in this case, is 6/1,000,000, which gives an MTTF of 0.000006. We decided this was the minimum acceptable MTTF since the graph was mostly in the "accept" region, with very little in the "continue test" region. This way the system was acceptable for most of its lifespan.

![RDC with 6 failures](./media/rdc/mttf6.png)

Next, we ran several different "what-if" cases to see how the system would be affected by different acceptable failure values. First, we doubled the amount of acceptable failures, setting it to 12 failures per 1,000,000 calls. Because more failure is accepted, the system is very clearly accepted throughout its entire life. The calculation for MTTF, in this case, is 12/1,000,000, which gives an MTTF of 0.000012.

![RDC with 12 failures](./media/rdc/mttf12.png)

The last case we ran was halving the number of acceptable failures, setting it to 3 per 1,000,000 calls. The calculation for MTTF, in this case, is 3/1,000,000, which gives an MTTF of 0.000003. By reducing the number of acceptable failures, the system is not as acceptable as it previously was. Thankfully, the system does not enter into "reject" region, but it is firmly in the "continue testing" region. If this was the desired amount of failure, the system would require more testing. 

![RDC with 3 failures](./media/rdc/mttf3.png)

ADD MORE ANALYSIS HERE

# Comparison of Results

After comparing both sets of data, it seemed that the RDC testing gave us much more information with regards to the reliability metrics. For the RGT and RDC, the Failure report 10 file was used. The results indicate that the system becomes more reliable as more tests occur. This is easier to see in the RDC tests as it is shown to quickly enter the acceptable range and continue into this range instead of entering the rejection range. This is also shown in the RGT as it starts with quite a few failures over time but then after around interval 30, the amount of failures is reduced. Going into more depth in these tests allowed us to more clearly see whether to accept or reject this system. Running a few of the model results allowed us to see the trends that this data created. It showed that there was an increase in problems on its way to middle of our run time but then the number of failures over times decreased. This shows that the results of each test agree that the data is becoming more reliable as it continues to run. 
This is also reflected in the RDC as it starts in the yellow zone but then will transfer to the green zone as the time between failures increases. You can also notice the failure numbers increasing around the middle of its run time as well. In every test run with the RDC, it never reached the red zone which would mean that we have an unreliable system. If we wanted to continue to improve this system shown in the 10th failure report, the work should be focused on its startup and short-term operation as that is where most of the problem areas are being shown in the tests.

# Discussion on Similarity and Differences of the Two Techniques

Similarities:
These techniques both allow you to analyze the reliability of a given system over a given amount of time. Using the same type of failure count data with different data sets, you are able to produce two different reliability metrics given the testing results.

Differences:
These techniques analyze two separate metrics while using them in different ways to determine the reliability. The RGT is used to estimate how reliable a system is over time as it grows which can be important to help determine what types of standards you want during development and testing of the system. The RDC allows you to look at the system to determine the acceptability rates for the operational stage of the system. 

# How the team work/effort was divided and managed

Because of the issues running the software on Mac, we split the lab into two parts. The reliability growth testing portion was handled by our two group members who use Windows. The second half, using the RDC was handled by the two members who used Mac, using the RDC-11 Excel sheet. We met in the lab time and on discord over the rest of the week to discuss our findings.

# Difficulties encountered, challenges overcome, and lessons learned

This lab had many difficulties in it that were experienced by our team. The first difficulty was with SRTAT. Half of our team uses MacOS and they were not able to run this program. We then moved on to C-SFRAT, which also did not work on MacOS. Neither software worked with the data provided, even when converted to a CSV file rather than the given DAT file. There was no information on what format the data needed to be in, leaving us to flounder and guess what the programs were looking for. We later found that if we moved the contents of a file to a .csv provided in the sample section of C-SFRAT, it would work and prodice the needed graphs.

Moving on to the second half of the lab, even more difficulties were encountered. Once again SRTAT does not work on MacOS. Attemting to use RDC-11 also provided a new array of issues. The first one was the lack of instruction other than "fill in the data." There is no documentation on how the sheet works either from the createors or the teaching team. Every single set of failure data is in a different format, so we had no idea which one to use or how to use it. When we attempted to choose one and import the data into RDC-11, we found that the values we had were locked in the spreadsheet.

# Comments/feedback on the lab itself

This lab was incredibly frustrating and diffcult to complete. The expectations were incredibly unclear, and even when the students requested clarification, none was offered. This lab forced us to think outside of the box and find new ways to solve problems the course failed to prepare us for. In future years, reconsider this entire assignment and the software used. All of the software was incredibly outdated and was difficult if not impossible to use on a modern operating system. As well, it is important to point out that these programs are not really used in the working world which makes this lab not very useful. There were also many errors in each program that we used which made if feel like we were looking at the first lab again. For example, on C-SFRAT, if you press the Run Estimations button and then close the loader, it will grey out the button until you restart the program.
